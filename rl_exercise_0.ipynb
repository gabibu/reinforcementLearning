{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_exercise_0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4mBLnVgDYAA",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 0 - PyTorch Tutorial\n",
        "## Yandex Data Science School RL Course 2019\n",
        "\n",
        "In this exercise you will get up to speed with pyTorch a modern dynamic deep learning framework by Facebook.\n",
        "\n",
        "We will use it for the rest of the book as our platform for training Deep Neural Network.\n",
        "\n",
        "Plenty of tutorials are available online. For example the official one are here https://pytorch.org/tutorials/#getting-started and the official documentations https://pytorch.org/docs/stable/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sPBzFJBKSyT",
        "colab_type": "text"
      },
      "source": [
        "## Part 1. Simple Graph\n",
        "Create a DNN with the following layers:\n",
        "1. Linear `n_input` to 5\n",
        "2. Relu\n",
        "3. Linear 5 to 20\n",
        "4. Relu\n",
        "5. Linear 20 to `num_classes`\n",
        "5. Dropout with probability `dropout_prob`\n",
        "6. soft max\n",
        "\n",
        "hints:\n",
        "1.use `nn.Sequential` to create a cascade of layers\n",
        "2. use `nn.X` to use layer of type `X`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1i4BLs7Ddfr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "cc501cd9-2744-4cfa-f8ee-e7a38031a8bd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class OurModule(nn.Module):\n",
        "    def __init__(self, num_inputs, num_classes, dropout_prob=0.3):\n",
        "        super(OurModule, self).__init__()\n",
        "        self.pipe = nn.Sequential(\n",
        "            nn.Linear(num_inputs, 5),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(5, 20),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(20, num_classes),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Softmax(1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pipe(x)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    net = OurModule(num_inputs=2, num_classes=3)\n",
        "    print(net)\n",
        "    v = torch.FloatTensor([[2, 3]])\n",
        "    out = net(v)\n",
        "    print(out)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OurModule(\n",
            "  (pipe): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Linear(in_features=5, out_features=20, bias=True)\n",
            "    (3): ReLU(inplace)\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "    (5): Dropout(p=0.3)\n",
            "    (6): Softmax()\n",
            "  )\n",
            ")\n",
            "tensor([[0.3396, 0.3325, 0.3278]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi0IWOpSMCB7",
        "colab_type": "text"
      },
      "source": [
        "## Part 2. Using GPU in Colab\n",
        "Go to Runtime menu and select \"change runtime type\" and checkbox GPU.\n",
        "Run the following code to test if you have succeeded.\n",
        "\n",
        "```\n",
        "print(\"Cuda's availability is %s\" % torch.cuda.is_available())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9zfNlhkM0Lc",
        "colab_type": "code",
        "outputId": "cf6de575-5188-4e8b-99cd-c7e9103c1072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Cuda's availability is %s\" % torch.cuda.is_available())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda's availability is True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKUf8WftUp7M",
        "colab_type": "text"
      },
      "source": [
        "In order to copy a tensor to the device you can user `.to('cuda')` or just `.cuda`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLCbQ6YHU8jI",
        "colab_type": "code",
        "outputId": "030d0ddb-e5d0-4a3c-88ec-1135d135b141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "v = torch.FloatTensor([[2, 3]])\n",
        "print(v.to('cuda'))\n",
        "print(v.cuda())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 3.]], device='cuda:0')\n",
            "tensor([[2., 3.]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aAPtHSBOccN",
        "colab_type": "text"
      },
      "source": [
        "## Part 3. Using TensorBoard in Colab\n",
        "\n",
        "Note: Ngrock is not necessary for someone who runs the notebook locally. If you run locally you can skip the ngrock issue.\n",
        "\n",
        "We would like to track learning. For this we have handy TensorBoard GUI.=\n",
        "\n",
        "If we work in Colab, since tensorboard is a server-client application, we need to create a network tunnel to access it through the colab virtual machine. See detailed explanation here https://medium.com/@tommytao_54597/use-tensorboard-in-google-colab-16b4bb9812a6 \n",
        "\n",
        "Use the following lines to use tensorboard in colab:\n",
        "\n",
        "```\n",
        "!pip install tensorboardX\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "tbc=TensorBoardColab(graph_path='./runs')\n",
        "```\n",
        "browse the printed url to access the tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doN_dizVIsck",
        "colab_type": "code",
        "outputId": "9a03dfad-d9c9-46f8-b209-8fff14f295fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "LOG_DIR = './log'\n",
        "\n",
        "!pip install tensorboardX\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "tbc=TensorBoardColab(graph_path=LOG_DIR)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.3)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n",
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://acf2d6ab.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFR22XOVSucb",
        "colab_type": "text"
      },
      "source": [
        "use tensorboardX SummaryWritter to create a write to the ./log/demo folder as log_dir.\n",
        "user the writter to log a sin cos and than values at each angle between -360 and 360.\n",
        "Visit the ngrok url of the TensorBoard to see your plots.\n",
        "It should look something like:\n",
        "\n",
        "![alt text](https://i.ibb.co/rGbpc5r/Screen-Shot-2019-03-22-at-14-09-30.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVBAq6gmELOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    writer = SummaryWriter(logdir=LOG_DIR)\n",
        "\n",
        "    funcs = {\"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan}\n",
        "\n",
        "    for angle in range(-360, 360):\n",
        "        angle_rad = angle * math.pi / 180\n",
        "        for name, fun in funcs.items():\n",
        "            val = fun(angle_rad)\n",
        "\n",
        "            writer.add_scalar(name, val)\n",
        "    writer.close()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t69Mp1jUQ1Q0",
        "colab_type": "text"
      },
      "source": [
        "## Part 4. MNIST Hello World\n",
        "\n",
        "1. Implement `Net` class below to create the following architecture:\n",
        "\n",
        "  1. Relu Conv2D 20x20x5 in_channels, out_channels, kernel_size, stride = 1,20,5,1\n",
        "  2. Max Pooled2D kernel_size, stride=2,2\n",
        "  3. Relu Conv2D in_channels, out_channels, kernel_size, stride = 20,50,5,1\n",
        "  4. Max Pooled2D kernel_size, stride=2,2\n",
        "  5. Fully connected in_features, out_features = 800, 500 (you will need to flat the tensor first)\n",
        "  6. Fully connected in_features, out_features = 500,10\n",
        "  \n",
        "  You can use the help class `Flatten`\n",
        "\n",
        "2. You can use the `get_loader` function to create a generator of batches.\n",
        "3. Don't forget to move the network to the gpu to speed up traning (`.to('cuda')`)\n",
        "4. Use SGD optimizer `optim.SGD`\n",
        "5. Use the following default parameters, you can use `get_args` to get an object with the default params, you will need the first line to make it work. https://github.com/spyder-ide/spyder/issues/3883\n",
        "  * batch_size = 65\n",
        "  * test_batch_size = 1000\n",
        "  * epochs = 10\n",
        "  * learning_rate = 0.01\n",
        "  * momentum = 0.5\n",
        "6. Implement a `train` and `test` function with the  signature below\n",
        "7. Use the writer=SummaryWriter to log training and test loss as well as test accuracy over time.\n",
        "8. Use the main method below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgXYZQWuQ0mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys; sys.argv=['']; del sys # without this argparse won't work properly. https://github.com/spyder-ide/spyder/issues/3883 \n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self, input):\n",
        "      return input.view(input.size(0), -1)\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.net = None\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.net(x)\n",
        "    \n",
        "def get_loader(train, batch_size, use_cuda):\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "  loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', \n",
        "                   train=train, \n",
        "                   download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, \n",
        "      shuffle=True, \n",
        "      **kwargs)\n",
        "  return loader\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, writer):\n",
        "  raise NotImplemented\n",
        "  \n",
        "def test(model, device, test_loader, epoch, writer):\n",
        "  raise NotImplemented\n",
        "  \n",
        "def get_args():\n",
        "  parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "  parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                      help='input batch size for training (default: 64)')\n",
        "  parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                      help='input batch size for testing (default: 1000)')\n",
        "  parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "                      help='number of epochs to train (default: 10)')\n",
        "  parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
        "                      help='learning rate (default: 0.01)')\n",
        "  parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
        "                      help='SGD momentum (default: 0.5)')\n",
        "  parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                      help='disables CUDA training')\n",
        "  parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                      help='random seed (default: 1)')\n",
        "  parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                      help='how many batches to wait before logging training status')\n",
        "\n",
        "  parser.add_argument('--save-model', action='store_true', default=False,\n",
        "                      help='For Saving the current Model')\n",
        "  args = parser.parse_args()\n",
        "  return args\n",
        "\n",
        "def main():\n",
        "    args = get_args()\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    print('use_cuda = {}'.format(use_cuda))\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    now = datetime.now()\n",
        "    writer = SummaryWriter(log_dir=LOG_DIR + '/nminst_' + now.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "    train_loader = get_loader(train=True, batch_size=args.batch_size, use_cuda=use_cuda)\n",
        "    test_loader = get_loader(train=False, batch_size=args.test_batch_size, use_cuda=use_cuda)\n",
        "    model = Net().to(device)\n",
        "    optimizer = # ADD THIS\n",
        "    \n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        print('epoch={}'.format(epoch))\n",
        "        train(model, device, train_loader, optimizer, epoch, writer)\n",
        "        test(model, device, test_loader, epoch, writer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUXi3MgpVUVO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}