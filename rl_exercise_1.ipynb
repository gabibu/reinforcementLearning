{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rl_exercise_1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SjEk4fxfxTSn","colab_type":"text"},"source":["# Exercise 1 - Grid World Hellow World\n","*Written by Dr. Hanan Shteingart*\n","## Yandex Data Science School RL Course 2019\n","\n","In this exercise you will get up to speed with gym.ai a testbed for RL agents.\n","\n","Use the following lines to use tensorboard in colab:\n","\n","```\n","!pip install tensorboardX\n","from tensorboardcolab import TensorBoardColab\n","tbc=TensorBoardColab(graph_path='./runs')\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"w6rephgWDO3N","colab_type":"text"},"source":["# Clif Climbing!\n","\n","> \"You send a robot to the wall in order to save the world from the white-walkers.  Your robot needs to go from \"S\" - your starting location to \"G\" where dragon glass is hidden. But notice the cliff! Don't fall there\"\n","\n","<img style=\"float: right;\" src=\"https://snag.gy/S63KeB.jpg\" alt=\"Drawing\" width=\"300\" />\n","\n","This is a standard undiscounted, episodic task, with start and goal states, and the usual actions causing movement up, down, right, and left. Reward is −1 on all transitions except those into the region marked “The Cliff.” Stepping into this region incurs a reward of −100 and sends the agent instantly\n","back to the start (example 6.6 in RL book by Sutton and Burto, edition 2)\n","\n","![alt text](https://i.snag.gy/v1hfRK.jpg)\n"]},{"cell_type":"markdown","metadata":{"id":"M400utE3EWRP","colab_type":"text"},"source":["## Part 1 - Implement the Environment (World) in the Gym Package\n","\n","Gym is a python package use to evaluate RL algorithm (read all about it here https://gym.openai.com/docs/#background-why-gym-2016)\n","\n","In Gym one can create an Environment `env` and iteract with it.\n","\n","Basic Concept:\n","* Action Space - space of possible actions - in our case top,bottom,left,right (total 4)\n","* Observation Space - space of possible observation. In our case, we have a \"fully observed MDP\" which means observation==state (12x4 grid, total 48) \n","\n","At first, the enviornment is reset via `env.reset()` which return the first observation.\n","Thereafter, there is a cycle where the environement receives an action chosen by the agent and returns a tuple with the resultant new observation, immidiate reward, whether the episode is over and an unused info variable (the tuple `new_state, reward, is_done, _` where the latter info is not used).\n","\n","The command env.render() displays the current state of the environment.\n","\n","**TASK** : Implement the cliff-walking environment as a class inheriting from `gym.Env`. You can use the following link to guide you:\n","* to learn more about spaces: http://gym.openai.com/docs/#spaces\n","* https://github.com/openai/gym/blob/master/docs/creating-environments.md\n","* https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html \n","* https://stackoverflow.com/questions/45068568/how-to-create-a-new-gym-environment-in-openai\n"]},{"cell_type":"code","metadata":{"id":"Pm89IDOfxn04","colab_type":"code","colab":{}},"source":["import gym\n","from gym import spaces\n","import numpy as np\n","class CliffWalking(gym.Env):\n","  CLIFF_REWARD = -100\n","  STEP_REWARD = -1\n","  N_ROWS = 4\n","  N_COLS = 12\n","  START = (0,0)\n","  GOAL = (N_COLS-1,0)\n","  ACTION = {0:(-1,0), 1:(1,0), 2:(0,-1), 3:(0,1)}\n","  ACTION_NAME = {0:\"LEFT\", 1: \"RIGHT\", 2: \"DOWN\", 3:\"UP\"}\n","  CLIFF_ROW = 0\n","  def __init__(self):\n","    super().__init__()\n","    self.state = None\n","    self.action_space = #TODO\n","    self.observation_space = #TODO\n","    self.last_action = None\n","    self.last_reward = None\n","    self.is_done = None\n","  \n","  def step(self, action):\n","    #TODO\n","    return state, reward, is_done, None\n","\n","  def reset(self):\n","    #TODO\n","    return state\n","  \n","  def state_to_int(self, state):\n","    # helper function returns an integer representative of the coordinate\n","    return np.ravel_multi_index(state, (self.N_COLS, self.N_ROWS))\n","  \n","  def render(self):\n","    #prints the state of the environment\n","    #TODO"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P_VxLrSJVpEs","colab_type":"code","outputId":"dcac8e0c-054b-44b2-b154-7e2d931aef06","executionInfo":{"status":"ok","timestamp":1555604477250,"user_tz":-180,"elapsed":2222,"user":{"displayName":"Hanan Shteingart","photoUrl":"https://lh6.googleusercontent.com/-jS1D3g51aU8/AAAAAAAAAAI/AAAAAAABEdI/FmJi29ZHYV4/s64/photo.jpg","userId":"11934753179242311747"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["#demo run\n","env = CliffWalking()\n","env.reset()\n","env.render()\n","env.step(0)\n","env.render()\n","env.step(3)\n","env.render()\n","env.step(3)\n","env.render()\n","env.step(3)\n","env.render()\n","env.step(3)\n","env.render()\n","env.step(3)\n","env.render()\n","env.step(3)\n","env.render()\n","\n","env.state\n","for _ in range(11):\n","  env.step(1)\n","  env.render()\n","env.step(2)\n","env.render()\n","env.step(2)\n","env.render()\n","env.step(2)\n","env.render()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["a=None r=None done=False\n","____________\n","____________\n","____________\n","*CLIFFCLIFFC\n","\n","a=LEFT r=-1 done=False\n","____________\n","____________\n","____________\n","*CLIFFCLIFFC\n","\n","a=UP   r=-1 done=False\n","____________\n","____________\n","*___________\n","SCLIFFCLIFFC\n","\n","a=UP   r=-1 done=False\n","____________\n","*___________\n","____________\n","SCLIFFCLIFFC\n","\n","a=UP   r=-1 done=False\n","*___________\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=UP   r=-1 done=False\n","*___________\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=UP   r=-1 done=False\n","*___________\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=UP   r=-1 done=False\n","*___________\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","_*__________\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","__*_________\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","___*________\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","____*_______\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","_____*______\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","______*_____\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","_______*____\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","________*___\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","_________*__\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","__________*_\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=RIGHT r=-1 done=False\n","___________*\n","____________\n","____________\n","SCLIFFCLIFFC\n","\n","a=DOWN r=-1 done=False\n","____________\n","___________*\n","____________\n","SCLIFFCLIFFC\n","\n","a=DOWN r=-1 done=False\n","____________\n","____________\n","___________*\n","SCLIFFCLIFFC\n","\n","a=DOWN r=0 done=True\n","____________\n","____________\n","____________\n","SCLIFFCLIFF*\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hU76L3MhMmnr","colab_type":"text"},"source":["# Q-learning\n","Recall the Q learning update rule:\n","\n","![alt text](https://cdn-images-1.medium.com/max/1280/0*BPeyfQgVvGtB7E5U.png)\n","\n","**TASK**: Implement an on-policy Q-learning agent class `Qlearn` with epsilon greedy decision rule where $\\epsilon=0.1$ (with probability $1-\\epsilon$ choose optimal action, otherwise random).\n","\n","**TASK** implement a simulation method `play_episode(agent, env)` to play the agent against the environment and return the total reward.\n","\n","** TASK** run the play_episode for N_EPISODES = 1000 and log the performance using tensorboard. \n","\n","![alt text](https://i.snag.gy/wRJCYG.jpg)\n","\n","** TASK** Plot the resultant optimal policy learned using `plt.quiver`, e.g.\n","\n","![alt text](https://i.snag.gy/O915hf.jpg)\n","\n","**OPTIONAL**: run the learning several times and plot the distribution of reward vs time (`plt.fill_between`)\n","\n","**OPTIONAL**: study the effect of learning rate, exploration, gamma.\n","![alt text](https://i.snag.gy/TJE4Gx.jpg)\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"YIDrJq_NxtoL","colab_type":"code","colab":{}},"source":["import gym\n","import collections\n","from tensorboardX import SummaryWriter\n","import numpy as np\n","GAMMA = 1 # no discounting in this world\n","ALPHA = 0.1\n","EPSILON = 0.1\n","\n","class Qlearning:\n","    def __init__(self, n_s, n_a, gamma=GAMMA, alpha=ALPHA, epsilon=EPSILON):\n","      #TODO      \n","    def act(self, state):\n","      #TODO\n","      return action\n","    \n","    def update(self, state, reward):\n","      #TODO\n","      self.last_state = state\n","      self.last_reward = reward\n","    \n","    def q_learn_update(self, state, action, reward, new_state):\n","      #TODO\n","    \n","    def step(self, state, reward):\n","      self.t +=1\n","      self.update(state, reward)\n","      action = self.act(state)      \n","      return action\n","    \n","    def random_action(self):\n","      #TODO\n","      return action\n","    \n","    def optimal_action(self,state):\n","      #TODO\n","      return action\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1tD1KOLLGuxm","colab_type":"code","colab":{}},"source":["!pip install tensorboardX\n","from tensorboardcolab import TensorBoardColab\n","LOG_DIR = './runs'\n","tbc=TensorBoardColab(graph_path=LOG_DIR)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pnYvZNoOJCwM","colab_type":"code","colab":{}},"source":["MAX_EPISODES = 1000\n","RENDER = False\n","def play_episode(agent, env, render=False):\n","  #TODO\n","\n","def run(env_args={}, agent_args={}):\n","  env = CliffWalking(**env_args)\n","  agent = Qlearning(n_a= env.action_space.n, n_s=env.observation_space.n, **agent_args)\n","  writer = SummaryWriter(comment=\"cliff_walking_q_learning\", log_dir=LOG_DIR+\"/\"+ str(env_args)+str(agent_args))\n","  best_reward = 0\n","  run_reward = 0\n","  for i_episode in range(MAX_EPISODES):\n","    #TODO\n","  writer.close()\n","  return agent, env, run_reward\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sy2KbIqj7EQG","colab_type":"code","colab":{}},"source":["# plot policy using quiver\n","%pylab inline\n","agent, env, reward  = run()\n","optimal_action = #TODO\n","optimal_move = #TODO\n","plt.quiver(np.arange(env.N_COLS), np.arange(env.N_ROWS), \n","           optimal_move[0]/2, optimal_move[1]/2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o1K9K4BvK0aw","colab_type":"code","colab":{}},"source":["#study effect of epsilon and alpha\n","for eps in [0, 0.01, 0.1]:\n","  for alpha in [0.01, 0.1]:\n","    _,_,reward = run(agent_args={'epsilon': eps, 'alpha': alpha})\n","    print('eps={}, alpha={}: reward={}'.format(eps, alpha, reward))"],"execution_count":0,"outputs":[]}]}