{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_exercise_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjEk4fxfxTSn",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 1 - Grid World Hellow World\n",
        "*Written by Dr. Hanan Shteingart*\n",
        "## Yandex Data Science School RL Course 2019\n",
        "\n",
        "In this exercise you will get up to speed with gym.ai a testbed for RL agents.\n",
        "\n",
        "Use the following lines to use tensorboard in colab:\n",
        "\n",
        "```\n",
        "!pip install tensorboardX\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "tbc=TensorBoardColab(graph_path='./runs')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6rephgWDO3N",
        "colab_type": "text"
      },
      "source": [
        "# Clif Climbing!\n",
        "\n",
        "> \"You send a robot to the wall in order to save the world from the white-walkers.  Your robot needs to go from \"S\" - your starting location to \"G\" where dragon glass is hidden. But notice the cliff! Don't fall there\"\n",
        "\n",
        "<img style=\"float: right;\" src=\"https://snag.gy/S63KeB.jpg\" alt=\"Drawing\" width=\"300\" />\n",
        "\n",
        "This is a standard undiscounted, episodic task, with start and goal states, and the usual actions causing movement up, down, right, and left. Reward is −1 on all transitions except those into the region marked “The Cliff.” Stepping into this region incurs a reward of −100 and sends the agent instantly\n",
        "back to the start (example 6.6 in RL book by Sutton and Burto, edition 2)\n",
        "\n",
        "![alt text](https://i.snag.gy/v1hfRK.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M400utE3EWRP",
        "colab_type": "text"
      },
      "source": [
        "## Part 1 - Implement the Environment (World) in the Gym Package\n",
        "\n",
        "Gym is a python package use to evaluate RL algorithm (read all about it here https://gym.openai.com/docs/#background-why-gym-2016)\n",
        "\n",
        "In Gym one can create an Environment `env` and iteract with it.\n",
        "\n",
        "Basic Concept:\n",
        "* Action Space - space of possible actions - in our case top,bottom,left,right (total 4)\n",
        "* Observation Space - space of possible observation. In our case, we have a \"fully observed MDP\" which means observation==state (12x4 grid, total 48) \n",
        "\n",
        "At first, the enviornment is reset via `env.reset()` which return the first observation.\n",
        "Thereafter, there is a cycle where the environement receives an action chosen by the agent and returns a tuple with the resultant new observation, immidiate reward, whether the episode is over and an unused info variable (the tuple `new_state, reward, is_done, _` where the latter info is not used).\n",
        "\n",
        "The command env.render() displays the current state of the environment.\n",
        "\n",
        "**TASK** : Implement the cliff-walking environment as a class inheriting from `gym.Env`. You can use the following link to guide you:\n",
        "* to learn more about spaces: http://gym.openai.com/docs/#spaces\n",
        "* https://github.com/openai/gym/blob/master/docs/creating-environments.md\n",
        "* https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html \n",
        "* https://stackoverflow.com/questions/45068568/how-to-create-a-new-gym-environment-in-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm89IDOfxn04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class CliffWalking(gym.Env):\n",
        "    CLIFF_REWARD = -100\n",
        "    STEP_REWARD = -1\n",
        "    N_ROWS = 4\n",
        "    N_COLS = 12\n",
        "    START = (N_ROWS-1, 0)\n",
        "    GOAL = (N_ROWS - 1, N_COLS-1)\n",
        "    ACTION = {0: (0, -1), 1:(0, 1) , 2:(-1, 0) , 3: (1, 0)}\n",
        "    ACTION_NAME = {0: \"LEFT\", 1: \"RIGHT\", 2: \"DOWN\", 3: \"UP\"}\n",
        "    CLIFF_ROW = 0\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.state = None\n",
        "        self.action_space =  list(self.ACTION.keys())\n",
        "\n",
        "        self.observation_space =  np.full(shape=self.N_ROWS * self.N_COLS, fill_value=self.STEP_REWARD)\n",
        "\n",
        "        for clif_index in range(1, self.N_COLS-1):\n",
        "            actual_index = self.state_to_int((self.N_ROWS-1, clif_index))\n",
        "            self.observation_space[actual_index] = self.CLIFF_REWARD\n",
        "\n",
        "\n",
        "\n",
        "        self.last_action = None\n",
        "        self.last_reward = None\n",
        "        self.is_done = None\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        current_action_values = self.ACTION[action]\n",
        "\n",
        "\n",
        "        next_row = max(0, self.state[0] + current_action_values[0])\n",
        "        next_row = min(next_row, self.N_ROWS -1)\n",
        "\n",
        "        next_col = max(0, self.state[1] + current_action_values[1])\n",
        "        next_col = min(next_col, self.N_COLS - 1)\n",
        "\n",
        "        self.state = (next_row, next_col)\n",
        "\n",
        "        self.last_action = self.ACTION_NAME[action]\n",
        "        self.last_reward = self.observation_space[self.state_to_int(self.state)]\n",
        "\n",
        "        self.is_done = self.state == self.GOAL or self.last_reward == self.CLIFF_REWARD\n",
        "        reward = self.last_reward\n",
        "\n",
        "        return (self.state_to_int(self.state), reward, self.is_done, {})\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.START\n",
        "        self.last_action = None\n",
        "        self.last_reward = None\n",
        "        self.is_done = None\n",
        "\n",
        "        return self.state_to_int(self.state)\n",
        "\n",
        "    def state_to_int(self, state):\n",
        "        # helper function returns an integer representative of the coordinate\n",
        "        return np.ravel_multi_index(state, (self.N_ROWS, self.N_COLS))\n",
        "\n",
        "    def render(self):\n",
        "        print('***************************')\n",
        "        print('a ={0} r={1} done={2}'\n",
        "              .format(self.last_action, self.last_reward, self.is_done))\n",
        "\n",
        "        for row_index in  range(self.N_ROWS):\n",
        "            row_vals =[]\n",
        "            for col_index in range(self.N_COLS):\n",
        "\n",
        "                index = self.state_to_int((row_index, col_index))\n",
        "                value = self.observation_space[index]\n",
        "\n",
        "                if(row_index, col_index) == self.state:\n",
        "                    row_val = 'P'\n",
        "                elif (row_index, col_index) == self.START:\n",
        "                    row_val = 'S'\n",
        "                elif (row_index, col_index) == self.GOAL:\n",
        "                    row_val = 'G'\n",
        "                else:\n",
        "                    if value ==  self.CLIFF_REWARD:\n",
        "                        row_val = 'C'\n",
        "                    else:\n",
        "                        row_val = 'R'\n",
        "\n",
        "                row_vals.append(row_val)\n",
        "\n",
        "            print(','.join(row_vals))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_VxLrSJVpEs",
        "colab_type": "code",
        "outputId": "38e9f223-5862-4388-e1d7-1ed6413b17e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2363
        }
      },
      "source": [
        "#demo run\n",
        "env = CliffWalking()\n",
        "env.reset()\n",
        "env.render()\n",
        "env.step(0)\n",
        "env.render()\n",
        "env.step(3)\n",
        "env.render()\n",
        "env.step(3)\n",
        "env.render()\n",
        "env.step(3)\n",
        "env.render()\n",
        "env.step(3)\n",
        "env.render()\n",
        "env.step(3)\n",
        "env.render()\n",
        "env.step(3)\n",
        "env.render()\n",
        "\n",
        "env.state\n",
        "for _ in range(11):\n",
        "  env.step(1)\n",
        "  env.render()\n",
        "env.step(2)\n",
        "env.render()\n",
        "env.step(2)\n",
        "env.render()\n",
        "env.step(2)\n",
        "env.render()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***************************\n",
            "a =None r=None done=None\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "P,C,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =LEFT r=-1 done=False\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "P,C,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =UP r=-1 done=False\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "P,C,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =UP r=-1 done=False\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "P,C,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =UP r=-1 done=False\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "P,C,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =UP r=-1 done=False\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "P,C,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =UP r=-1 done=False\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "P,C,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =UP r=-1 done=False\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "P,C,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =RIGHT r=-100 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,P,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =RIGHT r=-100 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,P,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =RIGHT r=-100 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,P,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =RIGHT r=-100 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,C,P,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =RIGHT r=-100 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,C,C,P,C,C,C,C,C,G\n",
            "***************************\n",
            "a =RIGHT r=-100 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,C,C,C,P,C,C,C,C,G\n",
            "***************************\n",
            "a =RIGHT r=-100 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,C,C,C,C,P,C,C,C,G\n",
            "***************************\n",
            "a =RIGHT r=-100 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,C,C,C,C,C,P,C,C,G\n",
            "***************************\n",
            "a =RIGHT r=-100 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,C,C,C,C,C,C,P,C,G\n",
            "***************************\n",
            "a =RIGHT r=-100 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,C,C,C,C,C,C,C,P,G\n",
            "***************************\n",
            "a =RIGHT r=-1 done=True\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,C,C,C,C,C,C,C,C,P\n",
            "***************************\n",
            "a =DOWN r=-1 done=False\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,P\n",
            "S,C,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =DOWN r=-1 done=False\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,P\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,C,C,C,C,C,C,C,C,G\n",
            "***************************\n",
            "a =DOWN r=-1 done=False\n",
            "R,R,R,R,R,R,R,R,R,R,R,P\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "R,R,R,R,R,R,R,R,R,R,R,R\n",
            "S,C,C,C,C,C,C,C,C,C,C,G\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU76L3MhMmnr",
        "colab_type": "text"
      },
      "source": [
        "# Q-learning\n",
        "Recall the Q learning update rule:\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1280/0*BPeyfQgVvGtB7E5U.png)\n",
        "\n",
        "**TASK**: Implement an on-policy Q-learning agent class `Qlearn` with epsilon greedy decision rule where $\\epsilon=0.1$ (with probability $1-\\epsilon$ choose optimal action, otherwise random).\n",
        "\n",
        "**TASK** implement a simulation method `play_episode(agent, env)` to play the agent against the environment and return the total reward.\n",
        "\n",
        "** TASK** run the play_episode for N_EPISODES = 1000 and log the performance using tensorboard. \n",
        "\n",
        "![alt text](https://i.snag.gy/wRJCYG.jpg)\n",
        "\n",
        "** TASK** Plot the resultant optimal policy learned using `plt.quiver`, e.g.\n",
        "\n",
        "![alt text](https://i.snag.gy/O915hf.jpg)\n",
        "\n",
        "**OPTIONAL**: run the learning several times and plot the distribution of reward vs time (`plt.fill_between`)\n",
        "\n",
        "**OPTIONAL**: study the effect of learning rate, exploration, gamma.\n",
        "![alt text](https://i.snag.gy/TJE4Gx.jpg)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIDrJq_NxtoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter\n",
        "import numpy as np\n",
        "GAMMA = 1 # no discounting in this world\n",
        "ALPHA = 0.1\n",
        "EPSILON = 0.1\n",
        "\n",
        "class Qlearning:\n",
        "    def __init__(self, n_s, n_a, gamma=GAMMA, alpha=ALPHA, epsilon=EPSILON):\n",
        "        # self.q_values = np.full(shape = (n_s, n_a), fill_value= 0.001)\n",
        "        self.q_values = np.zeros(shape=(n_s, n_a))\n",
        "        self.epsilon = epsilon\n",
        "        self.last_action = None\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.last_state = None\n",
        "        self.last_reward = None\n",
        "\n",
        "        # TODO\n",
        "    def act(self, state):\n",
        "\n",
        "        action = self.random_action() if np.random.random()  < self.epsilon else self.optimal_action(state)\n",
        "        self.last_action = action\n",
        "        self.last_state = state\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update(self, state, reward):\n",
        "\n",
        "        self.q_learn_update(self.last_state, self.last_action, reward, state)\n",
        "\n",
        "\n",
        "        #self.last_reward = reward\n",
        "\n",
        "    def state_to_int(self, state):\n",
        "        # helper function returns an integer representative of the coordinate\n",
        "        return np.ravel_multi_index(state, (self.N_ROWS, self.N_COLS))\n",
        "\n",
        "    def q_learn_update(self, state, action, reward, new_state):\n",
        "\n",
        "\n",
        "\n",
        "        old_value = self.q_values[state, action]\n",
        "\n",
        "        next_max_value = np.max(self.q_values[new_state])\n",
        "\n",
        "        new_value = old_value * (1 - self.epsilon) + ((self.epsilon) * (reward + self.gamma * next_max_value))\n",
        "\n",
        "        self.q_values[state, action] = new_value\n",
        "\n",
        "    def step(self, state, reward):\n",
        "        self.t += 1\n",
        "        self.update(state, reward)\n",
        "        action = self.act(state)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def random_action(self):\n",
        "        action = np.random.choice(self.q_values.shape[1])\n",
        "        return action\n",
        "\n",
        "    def optimal_action(self, state):\n",
        "        state_actions = self.q_values[state]\n",
        "        action = np.argmax(state_actions)\n",
        "        return action\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tD1KOLLGuxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorboardcolab import TensorBoardColab\n",
        "LOG_DIR = './runs'\n",
        "tbc=TensorBoardColab(graph_path=LOG_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnYvZNoOJCwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_EPISODES = 1000\n",
        "RENDER = False\n",
        "def play_episode(agent, env, render=False):\n",
        "\n",
        "    current_state = env.reset()\n",
        "    total_reward = 0.0\n",
        "\n",
        "    is_done = False\n",
        "    while not is_done:\n",
        "\n",
        "        action = agent.act(current_state)\n",
        "\n",
        "        current_state, reward, is_done, _ = env.step(action)\n",
        "\n",
        "        agent.update(current_state, reward)\n",
        "\n",
        "        total_reward += reward\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "def run(env_args={}, agent_args={}):\n",
        "  env = CliffWalking(**env_args)\n",
        "  agent = Qlearning(n_a= len(env.action_space), n_s= env.observation_space.size, **agent_args)\n",
        "  writer = SummaryWriter(comment=\"cliff_walking_q_learning\", log_dir=LOG_DIR+\"/\"+ str(env_args)+str(agent_args))\n",
        "  best_reward = 0\n",
        "  run_reward = 0\n",
        "\n",
        "  for i_episode in range(MAX_EPISODES):\n",
        "    total_reward = play_episode(agent, env, False)\n",
        "    writer.add_scalar('reward', total_reward, i_episode)\n",
        "\n",
        "  writer.close()\n",
        "  return agent, env, run_reward\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy2KbIqj7EQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot policy using quiver\n",
        "%pylab inline\n",
        "agent, env, reward  = run()\n",
        "optimal_action = #TODO\n",
        "optimal_move = #TODO\n",
        "plt.quiver(np.arange(env.N_COLS), np.arange(env.N_ROWS), \n",
        "           optimal_move[0]/2, optimal_move[1]/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1K9K4BvK0aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#study effect of epsilon and alpha\n",
        "for eps in [0, 0.01, 0.1]:\n",
        "  for alpha in [0.01, 0.1]:\n",
        "    _,_,reward = run(agent_args={'epsilon': eps, 'alpha': alpha})\n",
        "    print('eps={}, alpha={}: reward={}'.format(eps, alpha, reward))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}