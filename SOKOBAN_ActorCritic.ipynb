{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole-ActorCritic.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L88ZFOfGsYB"
      },
      "source": [
        "Based on https://github.com/rlcode/reinforcement-learning/blob/master/2-cartpole/4-actor-critic/cartpole_a2c.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTdE6b3bqXeF"
      },
      "source": [
        "!sudo apt-get install python-numpy python-scipy python-matplotlib\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHR2Ui-lo8O"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg x11-utils\n",
        "!pip install -q 'gym==0.10.11'\n",
        "!pip install -q 'imageio==2.4.0'\n",
        "!pip install -q PILLOW\n",
        "!pip install -q 'pyglet==1.3.2'\n",
        "!pip install -q pyvirtualdisplay\n",
        "!pip install -q tf-agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4l2Co8-66Ch"
      },
      "source": [
        "!pip install colabgymrender\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPiEIP2rl6V5"
      },
      "source": [
        "import sys\n",
        "import gym\n",
        "import pylab\n",
        "import numpy as np\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from IPython import display\n",
        "from pyvirtualdisplay import Display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "d = Display()\n",
        "d.start()\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "directory = './video'\n",
        "from colabgymrender.recorder import Recorder\n",
        "env = Recorder(env, directory, fps = 3)\n",
        "state = env.reset()\n",
        "\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "for _ in range(1):\n",
        "    state, reward, done, info = env.step(env.action_space.sample()) \n",
        "    display.clear_output(wait=True)\n",
        "    img.set_data(env.render('rgb_array'))\n",
        "    plt.axis('off')\n",
        "    display.display(plt.gcf())\n",
        "    if done:\n",
        "        env.reset()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJCgJnx3g0yY"
      },
      "source": [
        "In the Cartpole environment:\n",
        "\n",
        "-   `observation` is an array of 4 floats: \n",
        "    -   the position and velocity of the cart\n",
        "    -   the angular position and velocity of the pole \n",
        "-   `reward` is a scalar float value\n",
        "-   `action` is a scalar integer with only two possible values:\n",
        "    -   `0` — \"move left\"\n",
        "    -   `1` — \"move right\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsWksf1WnmrF"
      },
      "source": [
        "# A2C(Advantage Actor-Critic) agent for the Cartpole\n",
        "class A2CAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        # if you want to see Cartpole learning, then change to True\n",
        "        self.render = False\n",
        "        self.load_model = False\n",
        "        # get size of state and action\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.value_size = 1\n",
        "\n",
        "        # These are hyper parameters for the Policy Gradient\n",
        "        self.discount_factor = 0.99\n",
        "        self.actor_lr = 0.001\n",
        "        self.critic_lr = 0.005\n",
        "\n",
        "        # create model for policy network\n",
        "        self.actor = self.build_actor()\n",
        "        self.critic = self.build_critic()\n",
        "\n",
        "        if self.load_model:\n",
        "            self.actor.load_weights(\"./video/cartpole_actor.h5\")\n",
        "            self.critic.load_weights(\"./video/cartpole_critic.h5\")\n",
        "\n",
        "    # approximate policy and value using Neural Network\n",
        "    # actor: state is input and probability of each action is output of model\n",
        "    def build_actor(self):\n",
        "        actor = Sequential()\n",
        "        actor.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "        actor.add(Dense(self.action_size, activation='softmax',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "        actor.summary()\n",
        "        # See note regarding crossentropy in cartpole_reinforce.py\n",
        "        actor.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=Adam(lr=self.actor_lr))\n",
        "        return actor\n",
        "\n",
        "    # critic: state is input and value of state is output of model\n",
        "    def build_critic(self):\n",
        "        critic = Sequential()\n",
        "        critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
        "                         kernel_initializer='he_uniform'))\n",
        "        critic.add(Dense(self.value_size, activation='linear',\n",
        "                         kernel_initializer='he_uniform'))\n",
        "        critic.summary()\n",
        "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
        "        return critic\n",
        "\n",
        "    # using the output of policy network, pick action stochastically\n",
        "    def get_action(self, state):\n",
        "        policy = self.actor.predict(state, batch_size=1).flatten()\n",
        "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
        "\n",
        "    # update policy network every episode\n",
        "    def train_model(self, state, action, reward, next_state, done):\n",
        "        target = np.zeros((1, self.value_size))\n",
        "        advantages = np.zeros((1, self.action_size))\n",
        "\n",
        "        value = self.critic.predict(state)[0]\n",
        "        next_value = self.critic.predict(next_state)[0]\n",
        "\n",
        "        if done:\n",
        "            advantages[0][action] = reward - value\n",
        "            target[0][0] = reward\n",
        "        else:\n",
        "            advantages[0][action] = reward + self.discount_factor * (next_value) - value\n",
        "            target[0][0] = reward + self.discount_factor * next_value\n",
        "\n",
        "        self.actor.fit(state, advantages, epochs=1, verbose=0)\n",
        "        self.critic.fit(state, target, epochs=1, verbose=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uby2p8-noM1"
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = A2CAgent(state_size, action_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXilZOvc1E8s"
      },
      "source": [
        "directory = './save_graph'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDWptQa9oDt8"
      },
      "source": [
        "import pylab\n",
        "scores, episodes = [], []\n",
        "EPISODES = 1000\n",
        "\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "\n",
        "    while not done:\n",
        "        if agent.render:\n",
        "            env.render()\n",
        "\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        # if an action make the episode end, then gives penalty of -100\n",
        "        reward = reward if not done or score == 499 else -100\n",
        "\n",
        "        agent.train_model(state, action, reward, next_state, done)\n",
        "\n",
        "        score += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            # every episode, plot the play time\n",
        "            score = score if score == 500.0 else score + 100\n",
        "            scores.append(score)\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, scores, 'b')\n",
        "            pylab.savefig(\"./video/cartpole_a2c.png\")\n",
        "            print(\"episode:\", e, \"  score:\", score)\n",
        "\n",
        "            # if the mean of scores of last 10 episode is bigger than 490\n",
        "            # stop training\n",
        "            if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
        "                sys.exit()\n",
        "\n",
        "    # save the model\n",
        "    if e % 50 == 0:\n",
        "        agent.actor.save_weights(\"./video/cartpole_actor.h5\")\n",
        "        agent.critic.save_weights(\"./video/cartpole_critic.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}