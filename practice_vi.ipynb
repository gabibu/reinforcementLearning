{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_vi.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X5s0dZjI07w",
        "colab_type": "text"
      },
      "source": [
        "# Exerice 2 - Value Iteration\n",
        "\n",
        "\n",
        "In this exerices you will learn how to estimate value of states and (state,action) as well as finding the optimal policty for model-based known MDPs:\n",
        "\n",
        "* You will start with a toy MDP taken from the wikipedia article on MDP (https://en.wikipedia.org/wiki/Markov_decision_process, see figure on the right)\n",
        "* You will then use the same technique to solve the famous grid-world problem called \"Frozen World\" (see below)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*Shamelessly stolen from \"Practical Deep RL\" course in Coursera. https://www.coursera.org/learn/practical-rl/ which is passed by Yandex people (see https://www.coursera.org/learn/practical-rl/home/info).*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMAvbbpLTfc4",
        "colab_type": "text"
      },
      "source": [
        "**NOTE**: \n",
        "In order to make the exerice work we need some auxilary code from `mdp.py`. Unfortunately it's a mess to add a file to colab environment (see https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab). \n",
        "\n",
        "I chose the simplest approach which is to download from the official yandex course github the necessary file. \n",
        "\n",
        "please **evaluate** (shift-enter) the cell below to download the necessary file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS7TkvFEPt68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get some additional code for the exercise. you should see mdp.py if you do %ls once this is done\n",
        "import urllib.request\n",
        "import os\n",
        "urllib.request.urlretrieve('https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring19/week02_value_based/mdp.py','mdp.py')\n",
        "assert os.path.os.path.exists('mdp.py'), \"file mdp.py is missing check why you could not download it\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0r3Ox5FHm4k",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: Toy Markov decision process\n",
        "\n",
        "This week's methods are all built to solve __M__arkov __D__ecision __P__rocesses. In the broadest sense, an MDP is defined by how it changes states and how rewards are computed.\n",
        "\n",
        "State transition is defined by $P(s' |s,a)$ - how likely areare you to end at state $s'$ if you take action $a$ from state $s$. Now there's more than one way to define rewards, but we'll use $r(s,a,s')$ function for convenience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwrB__coHm4m",
        "colab_type": "text"
      },
      "source": [
        "For starters, let's define a simple MDP from this picture:\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Markov_Decision_Process.svg/800px-Markov_Decision_Process.svg.png' width=300px>\n",
        "_img by MistWiz (Own work) [Public domain], via Wikimedia Commons_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwL9I03jHm4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transition_probs = {\n",
        "    's0': {\n",
        "        'a0': {'s0': 0.5, 's2': 0.5},\n",
        "        'a1': {'s2': 1}\n",
        "    },\n",
        "    's1': {\n",
        "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "        'a1': {'s1': 0.95, 's2': 0.05}\n",
        "    },\n",
        "    's2': {\n",
        "        'a0': {'s0': 0.4, 's1': 0.6},\n",
        "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
        "    }\n",
        "}\n",
        "rewards = {\n",
        "    's1': {'a0': {'s0': +5}},\n",
        "    's2': {'a1': {'s0': -1}}\n",
        "}\n",
        "\n",
        "from mdp import MDP\n",
        "mdp = MDP(transition_probs, rewards, initial_state='s0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnbugO6FHm4r",
        "colab_type": "text"
      },
      "source": [
        "We can now use MDP just as any other gym environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOsoUYEyHm4t",
        "colab_type": "code",
        "outputId": "d08b5ea6-cfcc-4760-a361-ef044df4a57c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('initial state =', mdp.reset())\n",
        "next_state, reward, done, info = mdp.step('a1')\n",
        "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial state = s0\n",
            "next_state = s2, reward = 0.0, done = False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH0BzeD1Hm4v",
        "colab_type": "text"
      },
      "source": [
        "but it also has other methods that you'll need for Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQobtaQZHm4w",
        "colab_type": "code",
        "outputId": "336f341a-9784-4b4f-ba4f-38e8dd26c771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
        "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
        "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
        "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
        "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mdp.get_all_states = ('s0', 's1', 's2')\n",
            "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
            "mdp.get_next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
            "mdp.get_reward('s1', 'a0', 's0') =  5\n",
            "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BXDvc75Hm4z",
        "colab_type": "text"
      },
      "source": [
        "### Value Iteration\n",
        "\n",
        "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
        "\n",
        "Here's the pseudo-code for VI:\n",
        "\n",
        "---\n",
        "\n",
        "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
        "\n",
        "`2.` For $i=0, 1, 2, \\dots$\n",
        " \n",
        "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvlGARjhHm4z",
        "colab_type": "text"
      },
      "source": [
        "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
        "\n",
        "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS3ydZm-Hm40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_action_value(mdp, state_values, state, action, gamma):\n",
        "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
        "    Q = 0.0\n",
        "\n",
        "    for next_state, next_state_value in state_values.items():\n",
        "        prob = mdp.get_transition_prob(state, action, next_state)\n",
        "        reward = mdp.get_reward( state, action, next_state)\n",
        "\n",
        "        Q += (prob) * (reward + gamma * next_state_value)\n",
        "\n",
        "    return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8gLfhMVHm43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
        "assert np.allclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
        "assert np.allclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDjEsLJUHm46",
        "colab_type": "text"
      },
      "source": [
        "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
        " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSU-seCCHm46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_new_state_value(mdp, state_values, state, gamma):\n",
        "    \"\"\" Computes next V(s) as per formula above. Please do not change state_values in process. \"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return 0\n",
        "\n",
        "    best = float('-inf')\n",
        "    for action in mdp.get_possible_actions(state):\n",
        "        value = get_action_value(mdp, state_values, state, action, gamma)\n",
        "\n",
        "        if value > best:\n",
        "            best = value\n",
        "\n",
        "    return best\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EgWRbqPHm4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_Vs_vopy = dict(test_Vs)\n",
        "assert np.allclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
        "assert np.allclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 0.69)\n",
        "assert test_Vs == test_Vs_vopy, \"please do not change state_values in get_new_state_value\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5nTXHAsHm5B",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's combine everything we wrote into a working value iteration algo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXtSi-BSHm5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2381
        },
        "outputId": "38e68cbf-0c3c-4682-8571-db74b62ad499"
      },
      "source": [
        "# parameters\n",
        "gamma = 0.9  # discount for MDP\n",
        "num_iter = 100  # maximum iterations, excluding initialization\n",
        "min_difference = 0.001  # stop VI if new values are this close to old values (or closer)\n",
        "\n",
        "# initialize V(s)\n",
        "state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(num_iter):\n",
        "\n",
        "    # Compute new state values using the functions you defined above.\n",
        "    # It must be a dict {state : new_V(state)}\n",
        "\n",
        "    new_state_values = {s: get_new_state_value(mdp, state_values, s, gamma) for s in state_values.keys()}\n",
        "    assert isinstance(new_state_values, dict)\n",
        "\n",
        "    # Compute difference\n",
        "    diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
        "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
        "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()), end='\\n\\n')\n",
        "    state_values = new_state_values\n",
        "\n",
        "    if diff < min_difference:\n",
        "        print(\"Terminated\")\n",
        "        break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter    0   |   diff: 3.50000   |   V(s0) = 0.000   V(s1) = 0.000   V(s2) = 0.000\n",
            "\n",
            "iter    1   |   diff: 1.89000   |   V(s0) = 0.000   V(s1) = 3.500   V(s2) = 0.000\n",
            "\n",
            "iter    2   |   diff: 1.70100   |   V(s0) = 0.000   V(s1) = 3.815   V(s2) = 1.890\n",
            "\n",
            "iter    3   |   diff: 1.13542   |   V(s0) = 1.701   V(s1) = 4.184   V(s2) = 2.060\n",
            "\n",
            "iter    4   |   diff: 0.73024   |   V(s0) = 1.854   V(s1) = 5.319   V(s2) = 2.871\n",
            "\n",
            "iter    5   |   diff: 0.61135   |   V(s0) = 2.584   V(s1) = 5.664   V(s2) = 3.540\n",
            "\n",
            "iter    6   |   diff: 0.54664   |   V(s0) = 3.186   V(s1) = 6.275   V(s2) = 3.989\n",
            "\n",
            "iter    7   |   diff: 0.49198   |   V(s0) = 3.590   V(s1) = 6.790   V(s2) = 4.535\n",
            "\n",
            "iter    8   |   diff: 0.42210   |   V(s0) = 4.082   V(s1) = 7.189   V(s2) = 4.959\n",
            "\n",
            "iter    9   |   diff: 0.36513   |   V(s0) = 4.463   V(s1) = 7.611   V(s2) = 5.352\n",
            "\n",
            "iter   10   |   diff: 0.32862   |   V(s0) = 4.816   V(s1) = 7.960   V(s2) = 5.717\n",
            "\n",
            "iter   11   |   diff: 0.29262   |   V(s0) = 5.145   V(s1) = 8.280   V(s2) = 6.032\n",
            "\n",
            "iter   12   |   diff: 0.26189   |   V(s0) = 5.429   V(s1) = 8.572   V(s2) = 6.323\n",
            "\n",
            "iter   13   |   diff: 0.23503   |   V(s0) = 5.691   V(s1) = 8.830   V(s2) = 6.584\n",
            "\n",
            "iter   14   |   diff: 0.21124   |   V(s0) = 5.925   V(s1) = 9.065   V(s2) = 6.817\n",
            "\n",
            "iter   15   |   diff: 0.19012   |   V(s0) = 6.135   V(s1) = 9.276   V(s2) = 7.028\n",
            "\n",
            "iter   16   |   diff: 0.17091   |   V(s0) = 6.325   V(s1) = 9.465   V(s2) = 7.218\n",
            "\n",
            "iter   17   |   diff: 0.15366   |   V(s0) = 6.496   V(s1) = 9.636   V(s2) = 7.388\n",
            "\n",
            "iter   18   |   diff: 0.13830   |   V(s0) = 6.649   V(s1) = 9.790   V(s2) = 7.542\n",
            "\n",
            "iter   19   |   diff: 0.12445   |   V(s0) = 6.788   V(s1) = 9.928   V(s2) = 7.680\n",
            "\n",
            "iter   20   |   diff: 0.11200   |   V(s0) = 6.912   V(s1) = 10.052   V(s2) = 7.805\n",
            "\n",
            "iter   21   |   diff: 0.10079   |   V(s0) = 7.024   V(s1) = 10.164   V(s2) = 7.917\n",
            "\n",
            "iter   22   |   diff: 0.09071   |   V(s0) = 7.125   V(s1) = 10.265   V(s2) = 8.017\n",
            "\n",
            "iter   23   |   diff: 0.08164   |   V(s0) = 7.216   V(s1) = 10.356   V(s2) = 8.108\n",
            "\n",
            "iter   24   |   diff: 0.07347   |   V(s0) = 7.297   V(s1) = 10.437   V(s2) = 8.190\n",
            "\n",
            "iter   25   |   diff: 0.06612   |   V(s0) = 7.371   V(s1) = 10.511   V(s2) = 8.263\n",
            "\n",
            "iter   26   |   diff: 0.05951   |   V(s0) = 7.437   V(s1) = 10.577   V(s2) = 8.329\n",
            "\n",
            "iter   27   |   diff: 0.05356   |   V(s0) = 7.496   V(s1) = 10.636   V(s2) = 8.389\n",
            "\n",
            "iter   28   |   diff: 0.04820   |   V(s0) = 7.550   V(s1) = 10.690   V(s2) = 8.442\n",
            "\n",
            "iter   29   |   diff: 0.04338   |   V(s0) = 7.598   V(s1) = 10.738   V(s2) = 8.491\n",
            "\n",
            "iter   30   |   diff: 0.03904   |   V(s0) = 7.641   V(s1) = 10.782   V(s2) = 8.534\n",
            "\n",
            "iter   31   |   diff: 0.03514   |   V(s0) = 7.681   V(s1) = 10.821   V(s2) = 8.573\n",
            "\n",
            "iter   32   |   diff: 0.03163   |   V(s0) = 7.716   V(s1) = 10.856   V(s2) = 8.608\n",
            "\n",
            "iter   33   |   diff: 0.02846   |   V(s0) = 7.747   V(s1) = 10.887   V(s2) = 8.640\n",
            "\n",
            "iter   34   |   diff: 0.02562   |   V(s0) = 7.776   V(s1) = 10.916   V(s2) = 8.668\n",
            "\n",
            "iter   35   |   diff: 0.02306   |   V(s0) = 7.801   V(s1) = 10.941   V(s2) = 8.694\n",
            "\n",
            "iter   36   |   diff: 0.02075   |   V(s0) = 7.824   V(s1) = 10.964   V(s2) = 8.717\n",
            "\n",
            "iter   37   |   diff: 0.01867   |   V(s0) = 7.845   V(s1) = 10.985   V(s2) = 8.738\n",
            "\n",
            "iter   38   |   diff: 0.01681   |   V(s0) = 7.864   V(s1) = 11.004   V(s2) = 8.756\n",
            "\n",
            "iter   39   |   diff: 0.01513   |   V(s0) = 7.881   V(s1) = 11.021   V(s2) = 8.773\n",
            "\n",
            "iter   40   |   diff: 0.01361   |   V(s0) = 7.896   V(s1) = 11.036   V(s2) = 8.788\n",
            "\n",
            "iter   41   |   diff: 0.01225   |   V(s0) = 7.909   V(s1) = 11.049   V(s2) = 8.802\n",
            "\n",
            "iter   42   |   diff: 0.01103   |   V(s0) = 7.922   V(s1) = 11.062   V(s2) = 8.814\n",
            "\n",
            "iter   43   |   diff: 0.00992   |   V(s0) = 7.933   V(s1) = 11.073   V(s2) = 8.825\n",
            "\n",
            "iter   44   |   diff: 0.00893   |   V(s0) = 7.943   V(s1) = 11.083   V(s2) = 8.835\n",
            "\n",
            "iter   45   |   diff: 0.00804   |   V(s0) = 7.952   V(s1) = 11.092   V(s2) = 8.844\n",
            "\n",
            "iter   46   |   diff: 0.00724   |   V(s0) = 7.960   V(s1) = 11.100   V(s2) = 8.852\n",
            "\n",
            "iter   47   |   diff: 0.00651   |   V(s0) = 7.967   V(s1) = 11.107   V(s2) = 8.859\n",
            "\n",
            "iter   48   |   diff: 0.00586   |   V(s0) = 7.973   V(s1) = 11.113   V(s2) = 8.866\n",
            "\n",
            "iter   49   |   diff: 0.00527   |   V(s0) = 7.979   V(s1) = 11.119   V(s2) = 8.872\n",
            "\n",
            "iter   50   |   diff: 0.00475   |   V(s0) = 7.984   V(s1) = 11.125   V(s2) = 8.877\n",
            "\n",
            "iter   51   |   diff: 0.00427   |   V(s0) = 7.989   V(s1) = 11.129   V(s2) = 8.882\n",
            "\n",
            "iter   52   |   diff: 0.00384   |   V(s0) = 7.993   V(s1) = 11.134   V(s2) = 8.886\n",
            "\n",
            "iter   53   |   diff: 0.00346   |   V(s0) = 7.997   V(s1) = 11.137   V(s2) = 8.890\n",
            "\n",
            "iter   54   |   diff: 0.00311   |   V(s0) = 8.001   V(s1) = 11.141   V(s2) = 8.893\n",
            "\n",
            "iter   55   |   diff: 0.00280   |   V(s0) = 8.004   V(s1) = 11.144   V(s2) = 8.896\n",
            "\n",
            "iter   56   |   diff: 0.00252   |   V(s0) = 8.007   V(s1) = 11.147   V(s2) = 8.899\n",
            "\n",
            "iter   57   |   diff: 0.00227   |   V(s0) = 8.009   V(s1) = 11.149   V(s2) = 8.902\n",
            "\n",
            "iter   58   |   diff: 0.00204   |   V(s0) = 8.011   V(s1) = 11.152   V(s2) = 8.904\n",
            "\n",
            "iter   59   |   diff: 0.00184   |   V(s0) = 8.014   V(s1) = 11.154   V(s2) = 8.906\n",
            "\n",
            "iter   60   |   diff: 0.00166   |   V(s0) = 8.015   V(s1) = 11.155   V(s2) = 8.908\n",
            "\n",
            "iter   61   |   diff: 0.00149   |   V(s0) = 8.017   V(s1) = 11.157   V(s2) = 8.909\n",
            "\n",
            "iter   62   |   diff: 0.00134   |   V(s0) = 8.019   V(s1) = 11.159   V(s2) = 8.911\n",
            "\n",
            "iter   63   |   diff: 0.00121   |   V(s0) = 8.020   V(s1) = 11.160   V(s2) = 8.912\n",
            "\n",
            "iter   64   |   diff: 0.00109   |   V(s0) = 8.021   V(s1) = 11.161   V(s2) = 8.913\n",
            "\n",
            "iter   65   |   diff: 0.00098   |   V(s0) = 8.022   V(s1) = 11.162   V(s2) = 8.915\n",
            "\n",
            "Terminated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sof85PlvHm5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8becf381-d153-4e09-8f20-804ca708b22b"
      },
      "source": [
        "print(\"Final state values:\", state_values)\n",
        "\n",
        "assert abs(state_values['s0'] - 8.032)  < 0.01\n",
        "assert abs(state_values['s1'] - 11.169) < 0.01\n",
        "assert abs(state_values['s2'] - 8.921)  < 0.01"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final state values: {'s0': 8.023123818663871, 's1': 11.163174814980803, 's2': 8.915559364985523}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTwfT7B2Hm5H",
        "colab_type": "text"
      },
      "source": [
        "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
        "\n",
        " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
        " \n",
        "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1JuT5UTHm5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
        "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return None\n",
        "\n",
        "    best_action = None\n",
        "    best_action_value = float('-inf')\n",
        "    for action in mdp.get_possible_actions(state):\n",
        "        action_value = get_action_value(mdp, state_values, state, action, gamma)\n",
        "\n",
        "        if action_value > best_action_value:\n",
        "            best_action_value = action_value\n",
        "            best_action = action\n",
        "\n",
        "    return best_action\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er8DJZvHHm5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
        "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
        "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewTPOIHsHm5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Measure agent's average reward\n",
        "\n",
        "s = mdp.reset()\n",
        "rewards = []\n",
        "for _ in range(10000):\n",
        "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
        "    rewards.append(r)\n",
        "\n",
        "print(\"average reward: \", np.mean(rewards))\n",
        "\n",
        "assert 0.85 < np.mean(rewards) < 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY6UktZvuGVK",
        "colab_type": "text"
      },
      "source": [
        "optional exercise: use the reterived policy and estimte it's value and compare to the optimal value that you have found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1rHpYUhHm5P",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: Frozen lake\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*S6CG3jyp5rGxMUGw_Bqr3Q.png)\n",
        "\n",
        "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
        "\n",
        "The surface is described using a grid like the following:\n",
        "```\n",
        "SFFF       (S: starting point, safe)\n",
        "FHFH       (F: frozen surface, safe)\n",
        "FFFH       (H: hole, fall to your doom)\n",
        "HFFG       (G: goal, where the frisbee is located)\n",
        "```\n",
        "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
        "\n",
        "see more info here: https://gym.openai.com/envs/FrozenLake-v0/ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntpuKbjDHm5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "76312d49-7f83-4f09-c7f2-8c64f9968e02"
      },
      "source": [
        "from mdp import FrozenLakeEnv\n",
        "mdp = FrozenLakeEnv(slip_chance=0)\n",
        "\n",
        "mdp.render()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*FFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5VR1eDPHm5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(mdp, state_values=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
        "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
        "    state_values = state_values or {s: 0 for s in mdp.get_all_states()}\n",
        "    for i in range(num_iter):\n",
        "\n",
        "        # Compute new state values using the functions you defined above.\n",
        "        # It must be a dict {state : new_V(state)}\n",
        "        new_state_values = {s: get_new_state_value(mdp, state_values, s, gamma) for s in state_values.keys()}\n",
        "        assert isinstance(new_state_values, dict)\n",
        "\n",
        "        # Compute difference\n",
        "        diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
        "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \" % (i, diff, new_state_values[mdp._initial_state]))\n",
        "\n",
        "        state_values = new_state_values\n",
        "        if diff < min_difference:\n",
        "            print(\"Terminated\")\n",
        "            break\n",
        "\n",
        "    return state_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utuuAdQuHm5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "6a5e88c0-01d3-487e-b615-9a84c4667796"
      },
      "source": [
        "state_values = value_iteration(mdp)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter    0   |   diff: 1.00000   |   V(start): 0.000 \n",
            "iter    1   |   diff: 0.90000   |   V(start): 0.000 \n",
            "iter    2   |   diff: 0.81000   |   V(start): 0.000 \n",
            "iter    3   |   diff: 0.72900   |   V(start): 0.000 \n",
            "iter    4   |   diff: 0.65610   |   V(start): 0.000 \n",
            "iter    5   |   diff: 0.59049   |   V(start): 0.590 \n",
            "iter    6   |   diff: 0.00000   |   V(start): 0.590 \n",
            "Terminated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P32-8VkpHm5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "outputId": "1c135285-55c2-4bbb-a603-fb1bc54b7d70"
      },
      "source": [
        "s = mdp.reset()\n",
        "mdp.render()\n",
        "for t in range(100):\n",
        "    a = get_optimal_action(mdp, state_values, s, gamma)\n",
        "    print(a, end='\\n\\n')\n",
        "    s, r, done, _ = mdp.step(a)\n",
        "    mdp.render()\n",
        "    if done: break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*FFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFF\n",
            "*HFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFF\n",
            "FHFH\n",
            "*FFH\n",
            "HFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFFF\n",
            "FHFH\n",
            "F*FH\n",
            "HFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H*FG\n",
            "\n",
            "right\n",
            "\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF*G\n",
            "\n",
            "right\n",
            "\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF*\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGXTnXngHm5e",
        "colab_type": "text"
      },
      "source": [
        "### Let's visualize!\n",
        "\n",
        "It's usually interesting to see what your algorithm actually learned under the hood. To do so, we'll plot state value functions and optimal actions at each VI step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2C8qQhwHm5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def draw_policy(mdp, state_values):\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    h, w = mdp.desc.shape\n",
        "    states = sorted(mdp.get_all_states())\n",
        "    V = np.array(#YOUR CODE HERE)\n",
        "    Pi = # YOUR CODE HERE. hint: use get_optimal action\n",
        "    plt.imshow(V.reshape(w, h), cmap='gray', interpolation='none', clim=(0, 1))\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(h) - .5)\n",
        "    ax.set_yticks(np.arange(w) - .5)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    Y, X = np.mgrid[0:4, 0:4]\n",
        "    a2uv = {'left': (-1, 0), 'down': (0, -1), 'right': (1, 0), 'up': (-1, 0)}\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
        "                     color='g', size=12, verticalalignment='center',\n",
        "                     horizontalalignment='center', fontweight='bold')\n",
        "            a = Pi[y, x]\n",
        "            if a is None: continue\n",
        "            u, v = a2uv[a]\n",
        "            plt.arrow(x, y, u * .3, -v * .3, color='m', head_width=0.1, head_length=0.1)\n",
        "    plt.grid(color='b', lw=2, ls='-')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_931gCDaHm5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(10):\n",
        "    print(\"after iteration %i\" % i)\n",
        "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
        "    draw_policy(mdp, state_values)\n",
        "\n",
        "# please ignore iter 0 at each step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP4ufDKqHm5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import sleep\n",
        "from IPython.display import clear_output\n",
        "\n",
        "mdp = FrozenLakeEnv(map_name='8x8', slip_chance=0.1)\n",
        "state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(30):\n",
        "    clear_output(True)\n",
        "    print(\"after iteration %i\" % i)\n",
        "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
        "    draw_policy(mdp, state_values)\n",
        "    sleep(0.5)\n",
        "\n",
        "# please ignore iter 0 at each step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN5tZqqjHm5p",
        "colab_type": "text"
      },
      "source": [
        "Massive tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycVVXakoHm5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdp = FrozenLakeEnv(slip_chance=0)\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done: break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert 1.0 <= np.mean(total_rewards) <= 1.0\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgz8xv_0Hm5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Measure agent's average reward\n",
        "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done: break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert 0.8 <= np.mean(total_rewards) <= 0.95\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZgYSR5aHm5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Measure agent's average reward\n",
        "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done: break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert 0.6 <= np.mean(total_rewards) <= 0.7\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mtKBBpMHm5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Measure agent's average reward\n",
        "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
        "        rewards.append(r)\n",
        "        if done: break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert 0.6 <= np.mean(total_rewards) <= 0.8\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}